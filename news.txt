Can Robots Help Solve the Reproducibility Crisis?
http://www.slate.com/articles/technology/future_tense/2016/06/automating_lab_research_could_help_resolve_the_reproducibility_crisis.html
2016-06-30
popular news
In recent years, there’s been increasing awareness of a problem across many scientific fields—the problem of reproducibility. Can experiments be repeated (or "reproduced") to arrive at the same result? Evidence is piling up that the answer, all too often, is no. This makes it difficult to know which results we can confidently rely on, and which are spurious.


Biomedical researchers lax about validating antibodies for experiments
http://www.nature.com/news/biomedical-researchers-lax-about-validating-antibodies-for-experiments-1.20192
2016-06-30
news article
Nearly one-third of junior scientists spend no time validating antibodies, even though accurate results depend on these reagents working as expected, according to the results of a survey reported today in BioTechniques. "This is quite alarming," says Matthias Uhlén, a protein researcher at the Royal Institute of Technology in Stockholm who heads an international working group on antibody validation, but who was not directly involved in the survey.


Reproducibility Data for: Direct and Indirect Welfare Chauvinism as Party Strategies
http://dx.doi.org/10.7910/DVN/ZLFP3A
2016-06-29
reproducible paper
Reproducibility material (data and code) for 'Direct and Indirect Welfare Chauvinism as Party Strategies: An Analysis of the Danish People’s Party', Scandinavian Political Studies.


Fire to the File Drawer: Sharing Reproducibility Data in an Online Age.
https://thewinnower.com/papers/4880-fire-to-the-file-drawer-sharing-reproducibility-data-in-an-online-age
2016-06-29
popular news
"It is entirely within the realm of possibility that the creation of a new publishing platform, focused on hosting formal replications, alongside these review style evaluations of method, would provide a new and more focused home for the type of discussion. Overall, implementing such a system would vastly improve the accessibility of research; both through providing links to peer reviewed replications which have not been filtered by the file drawer, and literally, in terms enabling an overview replication information out at a glance."


ReproZip 1.0.6 released
https://github.com/ViDA-NYU/reprozip/releases/tag/1.0.6
2016-06-25
ReproZip, reproducibility infrastructure
A new version of ReproZip has been released, adding some bugfixes and new commands related to distributed or server experiments.


Research Roundup: Improving reproducibility, the usefulness of clinical research and more
http://blogs.plos.org/plospodcasts/2016/06/23/research-roundup-improving-reproducibility-the-usefulness-of-clinical-research-and-more/
2016-06-24
news article
This week in science, academia and publishing for reproducibility.


Repeatability, Reproducibility, Separative Power and Subjectivity of Different Fish Morphometric Analysis Methods
http://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0157890
2016-06-21
reproducible paper
We compared the repeatability, reproducibility (intra- and inter-measurer similarity), separative power and subjectivity (measurer effect on results) of four morphometric methods frequently used in ichthyological research, the “traditional” caliper-based (TRA) and truss-network (TRU) distance methods and two geometric methods that compare landmark coordinates on the body (GMB) and scales (GMS).


Introducing ReproZip at SIGMOD
http://permalink.gmane.org/gmane.comp.db.dbworld/56283
2016-06-20
ReproZip
Remi Rampin and Fernando Chirigati of NYU will be presenting ReproZip at this year's SIGMOD ACM conference. ReproZip enables a researcher to create a compendium of his/her Linux experiment by automatically tracking and identifying all its required dependencies (data files, libraries, configuration files, etc.).


From Reproducibility to Accessibility
https://www.genomeweb.com/scan/reproducibility-accessibility
2016-06-15
news article
Jeremy Berg, the incoming editor-in-chief of Science magazine, will be grappling with a number of issues plaguing science and science publishing when he takes over that role, Retraction Watch's Shannon Palus writes. Berg has previously supported efforts to bolster reproducibility and transparency, Palus notes. He tells her that there are a number of efforts aimed at improving reproducibility underway at Science, but as he hasn't started the position yet — he's to take the helm in July — he needs to catch up on what's already been done. He says various issues could be behind the irreproducibility problem and, to be effective, any response has to be tailored to that issue.


Muddled meanings hamper efforts to fix reproducibility crisis
http://www.nature.com/news/muddled-meanings-hamper-efforts-to-fix-reproducibility-crisis-1.20076?WT.ec_id=NEWSDAILY-20160614
2016-06-14
news article
Researchers tease out different definitions of a crucial scientific term. A semantic confusion is clouding one of the most talked-about issues in research. Scientists agree that there is a crisis in reproducibility, but they can’t agree on what 'reproducibility' means.


Connectome hubs at resting state in children and adolescents: Reproducibility and psychopathological correlation.
http://www.ncbi.nlm.nih.gov/pubmed/27288820
2016-06-10
reproducible paper	
Functional brain hubs are key integrative regions in brain networks. Recently, brain hubs identified through resting-state fMRI have emerged as interesting targets to increase understanding of the relationships between large-scale functional networks and psychopathology. However, few studies have directly addressed the replicability and consistency of the hub regions identified and their association with symptoms. Here, we used the eigenvector centrality (EVC) measure obtained from graph analysis of two large, independent population-based samples of children and adolescents (7-15 years old; total N=652; 341 subjects for site 1 and 311 for site 2) to evaluate the replicability of hub identification. Subsequently, we tested the association between replicable hub regions and psychiatric symptoms. We identified a set of hubs consisting of the anterior medial prefrontal cortex and inferior parietal lobule/intraparietal sulcus (IPL/IPS). Moreover, lower EVC values in the right IPS were associated with psychiatric symptoms in both samples. Thus, low centrality of the IPS was a replicable sign of potential vulnerability to mental disorders in children. The identification of critical and replicable hubs in functional cortical networks in children and adolescents can foster understanding of the mechanisms underlying mental disorders.


Alan Turing Institute Symposium on Reproducibility for Data-Intensive Research - full programme
https://figshare.com/articles/Alan_Turing_Institute_Symposium_on_Reproducibility_for_Data-Intensive_Research_-_full_programme_6-7_April_2016/3422998
2016-06-10
reproducibility conference
Full programme and speaker biographies for the Alan Turing Institute Symposium on Data-Intensive Research, held 6-7 April 2016


What crisis? – the reproducibility crisis
https://thepsychologist.bps.org.uk/what-crisis-reproducibility-crisis
2016-06-09
news article
A huge audience of psychologists, students and researchers was drawn to the British Psychological Society debate in London about the reproducibility and replication crisis in psychology. After Brian Nosek and the Open Science Collaboration outlined the difficulty in reproducing psychological findings, the BPS, the Experimental Psychology Society and the Association of Heads of Psychology Departments hoped to host an upbeat and positive debate in the area. Ella Rhodes reports from a British Psychological Society debate.


Is there a reproducibility "crisis" in biomedical science? No, but there is a reproducibility problem
https://www.sciencebasedmedicine.org/is-there-a-reproducibility-crisis-in-biomedical-science-no-but-there-is-a-reproducibility-problem/
2016-06-06
popular news
Most scientists I know get a chuckle out of the Journal of Irreproducible Results (JIR), a humor journal that often parodies scientific papers. Back in the day, we used to chuckle at articles like "Any Eye for an Eye for an Arm and a Leg: Applied Dysfunctional Measurement" and "A Double Blind Efficacy Trial of Placebos, Extra Strength Placebos and Generic Placebos." Unfortunately, these days, reporting on science is giving the impression that the JIR is a little too close to the truth, at least when it comes to reproduciblity, so much so that the issue even has its own name and Wikipedia entry: Replication (or reproducibility) crisis.


SCIEX Announces High Throughput, Industrialized Omics Solutions at ASMS 2016
http://www.businesswire.com/news/home/20160606005088/en/SCIEX-Announces-High-Throughput-Industrialized-Omics-Solutions
2016-06-06
popular news
Advancements in Automation, Reproducibility and Robustness Enables Research to Scale like Never Before. SCIEX, a global leader in life science analytical technologies, today announced their latest proteomics solution advancements, which address the challenges of throughput, reproducibility and robustness faced by Academic Labs working to advance precision medicine. 


Elsewhere in Science: Open access, Dance Your Ph.D., and more
http://www.sciencemag.org/careers/2016/06/elsewhere-science-open-access-dance-your-phd-and-more
2016-06-03
news article
Here is the past week’s career-related news from across the Science family of publications.


Reproducible Research Resources for Research(ing) Parasites
http://blogs.biomedcentral.com/gigablog/2016/06/03/reproducible-research-resources-researching-parasites/
2016-06-03
news article
Two new research papers on scabies and tapeworms published today showcase a new collaboration with protocols.io. This demonstrates a new way to share scientific methods that allows scientists to better repeat and build upon these complicated studies on difficult-to-study parasites. It also highlights a new means of writing all research papers with citable methods that can be updated over time.


Reproducible research: A hunt for the truth
http://med.stanford.edu/news/all-news/2016/06/reproducible-research-a-hunt-for-the-truth.html
2016-06-03
popular news
Researchers write that "reproducibility,"replicability" and several other terms are not used consistently in scientific communication.


CU-Boulder graduate student wants transparent research practice policy
http://www.dailycamera.com/cu-news/ci_29949880/cu-boulder-graduate-student-wants-transparent-research-practice
2016-05-29
popular news
Inspired by a new movement to improve the transparency and reproducibility of research, graduate student John Lurquin wants the University of Colorado to adopt a campus-wide transparent research policy requiring academics to publish data and information about their experiments. Though reproducibility, or the ability to reproduce the results of an experiment, has always been on the minds of researchers, it's been getting more attention recently, thanks to several studies measuring the reliability of published research, said Lurquin, a doctoral student in the department of psychology and neuroscience and an outgoing student body president.


Let's see that again
http://www.rsc.org/chemistryworld/2016/05/pipeline-reproducibility-derek-lowe-lets-see-again
2016-05-27
news article
A few years ago, the topic of whether scientific papers are reproducible or not would have been an odd thing to see in a newspaper. But not any more: both the popular media and the journals themselves have been trying to deal with the topic, amid reports that far too many results can’t be replicated. Large scale efforts have begun to examine key papers in experimental psychology, among other areas. Reports from the biopharma industry about the numbers of interesting biology papers that don’t hold up have stirred alarm as well. But as far as I can tell, chemistry has largely escaped the current rounds of criticism.


Reproducibility: Crisis or Not?
http://blogs.sciencemag.org/pipeline/archives/2016/05/26/reproducibility-crisis-or-not
2016-05-26
news article
Here are the results of a Nature survey on reproducibility in the scientific literature. They themselves admit that it’s a "confusing snapshot", but it shows that we're still arguing about what "reproducibility" means. 52% of the responders (over 1500 scientists) said that there was "a significant crisis", though, so this issue is on people’s minds. Interestingly, chemists were among the most confidant in the literature of their own field (physics and engineering as well). At the same time, chemists had the highest proportion of respondents who said that they'd been unable to reproduce someone else's experiment. I don't think that's necessarily a contradiction, though. Chemistry is a field with lower barriers to replication than many others, and we also probably do more replications in general.


Money back guarantees for non-reproducible results?
http://dx.doi.org/10.1136/bmj.i2770
2016-05-24
news article
Money back guarantees are generally unheard of in biomedicine and healthcare. Recently, the US provider Geisenger Health System, in Pennsylvania, started a programme to give patients their money back if they were dissatisfied. That came as quite a surprise. Soon thereafter, the chief medical officer at Merck launched an even bigger one, proposing an "incentive-based approach" to non-reproducible results—what he termed a "reproducibility crisis" that "threatens the entire biomedical research enterprise."


1,500 scientists lift the lid on reproducibility
http://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970
2016-05-25
news article
More than 70% of researchers have tried and failed to reproduce another scientist's experiments, and more than half have failed to reproduce their own experiments. Those are some of the telling figures that emerged from Nature's survey of 1,576 researchers who took a brief online questionnaire on reproducibility in research. The data reveal sometimes-contradictory attitudes towards reproducibility. Although 52% of those surveyed agree that there is a significant 'crisis' of reproducibility, less than 31% think that failure to reproduce published results means that the result is probably wrong, and most say that they still trust the published literature.


Contextual sensitivity in scientific reproducibility
http://www.pnas.org/content/early/2016/05/18/1521897113.full
2016-05-24
reproducibility study	
Scientific progress requires that findings can be reproduced by other scientists. However, there is widespread debate in psychology (and other fields) about how to interpret failed replications. Many have argued that contextual factors might account for several of these failed replications. We analyzed 100 replication attempts in psychology and found that the extent to which the research topic was likely to be contextually sensitive (varying in time, culture, or location) was associated with replication success. This relationship remained a significant predictor of replication success even after adjusting for characteristics of the original and replication studies that previously had been associated with replication success (e.g., effect size, statistical power). We offer recommendations for psychologists and other scientists interested in reproducibility.


When Great Minds Think Unlike: Inside Science's 'Replication Crisis'
http://www.npr.org/2016/05/24/477921050/when-great-minds-think-unlike-inside-sciences-replication-crisis
2016-05-24
popular news
This week, Hidden Brain looks at the "replication crisis" through zooming in on one seminal paper that was the focus of two replication efforts: one succeeded in replicating the original finding, the other failed.


Research Quality Assurance: A Strategy for Improving Research Reproducibility
https://osf.io/29htc/
2016-05-24
reproducibility talk	
A poster by Rebecca Davies in the field of Veterinary Medicine.


Nature Reproducibility survey
https://figshare.com/articles/Nature_Reproducibility_survey/3394951
2016-05-24
reproducibility report, reproducibility study
Raw data from survey on reproducibility survey run by Nature Publishing Group November 2015, published in Nature June 2016


Webinar@AIMS Increasing Openness and Reproducibility in Agricultural Research
http://aims.fao.org/activity/blog/webinaraims-increasing-openness-and-reproducibility-agricultural-research
2016-05-19
reproducibility talk
There are many actions researchers can take to increase the openness and reproducibility of their work. This introductory webinar from the Center for Open Science is aimed at faculty, staff, and students involved in agricultural research. Participants will gain a foundation for incorporating reproducible, transparent practices into their current workflows.


Pain and Laboratory Animals: Publication Practices for Better Data Reproducibility and Better Animal Welfare
http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0155001
2016-05-12
reproducible paper
We report that publication guidelines focus more on other potential sources of bias in experimental results, under-appreciate the potential for pain and pain drugs to skew data, and thus mostly treat pain management as solely an animal welfare concern, in the jurisdiction of animal care and use committees. At the same time, animal welfare regulations do not include guidance on publishing animal data, even though publication is an integral part of the cycle of research and can affect the welfare of animals in studies building on published work, leaving it to journals and authors to voluntarily decide what details of animal use to publish. We suggest that journals, scientists and animal welfare regulators should revise current guidelines and regulations, on treatment of pain and on transparent reporting of treatment of pain, to improve this dual welfare and data-quality deficiency.


Research Reproducibility 2016
http://metrics.stanford.edu/node/research-reproducibility-2016
2016-05-12
research guide
This is a guide from Stanford University outlining methodology, tools, and resources for increasing reproducibility in science.


"Reproducibility Symposium" -- a recap of NYU's Reproducibility Symposium
http://theoryandpractice.org/2016/05/Reproducibility-Symposium/#.VyzammOgq5g
2016-05-03
reproducibility conference, popular news
Kyle Cranmer, a faculty member in NYU's physics department, distills and describes of the event of NYU's first Reproducibility Symposium on May 3, 2016. 


Reproducibility and Relative Validity of a Short Food Frequency Questionnaire in 9–10 Year-Old Children
http://www.mdpi.com/2072-6643/8/5/271
2016-05-07
reproducibility study
The aim of this study was to assess the reproducibility and validity of a non-quantitative 28-item food frequency questionnaire (FFQ). Children aged 9–10 years (n = 50) from three schools in Dunedin, New Zealand, completed the FFQ twice and a four-day estimated food diary (4DEFD) over a two-week period. Intraclass correlation coefficients (ICC) and Spearman’s correlation coefficients (SCC) were used to determine reproducibility and validity of the FFQ, respectively. 


John Oliver on scientific studies, statistical significance and reproducibility
https://www.youtube.com/watch?v=0Rnq1NpHdmw
2016-05-08
popular news
In his show Last Week Tonight, John Oliver discusses how and why media outlets so often report untrue or incomplete information as science.


Unconditional data sharing, plus peer review transparency, is key to research reproducibility
https://thewinnower.com/papers/4314-unconditional-data-sharing-plus-peer-review-transparency-is-key-to-research-reproducibility
2016-04-28
popular news, open access
Only mandatory Open Data, not Gold Open Access, will lead to more honest and more reproducible science. Open Science is these days largely about mandatory publishing in Open Access (OA), regardless of the costs to poorer scientists or the universities which already struggle to pay horrendous subscription fees. Meanwhile, publishers openly declare that the so-called Gold (author-pays) OA will be much more expensive than even current subscription rates, yet wealthy western institutions like the Dutch university network VSNU or the German Max Planck Society do not seem troubled by this at all. They seriously expect the publishing oligopoly of Elsevier, SpringerNature and Wiley to lower the costs for Gold OA later on, out of the goodness of their hearts (as this winter’s invitation-only Berlin12 OA conference suggests).


1st International Workshop on Reproducible Open Science (RepScience2016)
http://www.tpdl2016.org/repscience2016
2016-04-29
reproducibility conference
This Workshop aims at becoming a forum to discuss ideas and advancements towards the revision of current scientific communication practices in order to support Open Science, introduce novel evaluation schemes, and enable reproducibility. As such it candidates as an event fostering collaboration between (i) Library and information scientists working on the identification of new publication paradigms; (ii) ICT scientists involved in the definition of new technical solutions to these issues; (iii) scientists/researchers who actually conduct the research and demand tools and practices for Open Science. The expected results are advancements in the definition of the next generation scientific communication ecosystem, where scientists can publish research results (including the scientific article, the data, the methods, and any “alternative” product that may be relevant to the conducted research) in order to enable reproducibility (effective reuse and decrease of cost of science) and rely on novel scientific reward practices.


It bears repeating: how scientists are addressing the 'reproducibility problem'
http://theconversation.com/it-bears-repeating-how-scientists-are-addressing-the-reproducibility-problem-55369
2016-04-25
popular news
Recent reports in the Washington Post and the Economist, among others, raise the concern that relatively few scientists' experimental findings can be replicated. This is worrying: replicating an experiment is a main foundation of the scientific method. As scientists, we build on knowledge gained and published by others. We develop new experiments and questions based on the knowledge we gain from those published reports. If those papers are valid, our work is supported and knowledge advances. On the other hand, if published research is not actually valid, if it can’t be replicated, it delivers only an incidental finding, not scientific knowledge.


Transparency and Reproducibility in Economics Research
https://bids.berkeley.edu/resources/videos/transparency-and-reproducibility-economics-research
2016-04-22
reproducibility talk
There is growing interest in research transparency and reproducibility in economics and other scientific fields. We survey existing work on these topics within economics and discuss the evidence suggesting that publication bias, inability to replicate, and specification searching remain widespread problems in the discipline. We next discuss recent progress in this area, including improved research design, study registration and pre-analysis plans, disclosure standards, and open sharing of data and materials, and draw on experiences in both economics and other social sciences. We discuss areas where consensus is emerging on new practices as well as approaches that remain controversial and speculate about the most effective ways to make economics research more accurate, credible, and reproducible in the future.


Cancer Research Is Broken
http://www.slate.com/articles/health_and_science/future_tense/2016/0/biomedicine_facing_a_worse_replication_crisis_than_the_one_plaguing_psychology.html
2016-04-19
popular news
There’s a replication crisis in biomedicine—and no one even knows how deep it runs. Many science funders share Parker’s antsiness over all the waste of time and money. In February, the White House announced its plan to put $1 billion toward a similar objective—a “Cancer Moonshot” aimed at making research more techy and efficient. But recent studies of the research enterprise reveal a more confounding issue, and one that won’t be solved with bigger grants and increasingly disruptive attitudes. The deeper problem is that much of cancer research in the lab—maybe even most of it—simply can’t be trusted. The data are corrupt. The findings are unstable. The science doesn’t work.


A practical guide for improving transparency and reproducibility in neuroimaging research
http://biorxiv.org/content/early/2016/04/12/039354
2016-04-12
reproducibility guidelines
Recent years have seen an increase in alarming signals regarding the lack of replicability in neuroscience, psychology, and other related fields. To avoid a widespread crisis in neuroimaging research and consequent loss of credibility in the public eye, we need to improve how we do science. This article aims to be a practical guide for researchers at any stage of their careers that will help them make their research more reproducible and transparent while minimizing the additional effort that this might require. The guide covers three major topics in open science (data, code, and publications) and offers practical advice as well as highlighting advantages of adopting more open research practices that go beyond improved transparency and reproducibility.


Checklists vs. checkmate: Reproducibility key to premium surgery success
http://www.healio.com/ophthalmology/practice-management/news/print/ocular-surgery-news/%7Bff6cbf13-7a83-4e23-897c-5b5f4bbe2e1f%7D/checklists-vs-checkmate-reproducibility-key-to-premium-surgery-success
2016-04-03
popular news
Traditionally, checkmate is a position in the game of chess in which a player’s king is in check, without a way to remove the threat. The king cannot be captured, so the game ends when the king is checkmated. As a premium surgeon, no one ever wants to be checkmated at any stage of the surgical process, from preoperative to intraoperative to postoperative. Other forms of etymology have suggested checkmate to signify being “ambushed,” a feeling many of us have experienced in our surgical careers. A means to avoiding being a checkmated surgeon is creating “checklists” from the time of the first patient encounter until the final postoperative visit. The process of checklists can bring reproducibility to a surgical process that already yields successful outcomes in a premium surgeon’s practice.


Reproducibility in research results: the challenges of attributing reliability
http://blog.scielo.org/en/2016/03/31/reproducibility-in-research-results-the-challenges-of-attributing-reliability/
2016-03-31
popular news
Studies indicate, however, that more than half of the experiments involving clinical trials of new drugs and treatments are irreproducible. John Ioannidis at Stanford University, US, goes on saying that most of the search results is actually false. Ioannidis is the author of a mathematical model that predicts that the smaller the sample and less stringent are the experimental methodology, definitions, outcomes and statistical analysis, the greater the probability of error. Furthermore, studies that hold financial and other interests or of great impact are also more prone to false results.


Ten Major Errors in Obesity Research Discussed
http://www.newswise.com/articles/ten-major-errors-in-obesity-research-discussed
2016-03-30
popular news
A paper from investigators at the University of Alabama at Birmingham recently published in Obesity identifies several key statistical errors commonly seen in obesity research with discussions on how to identify and avoid making these mistakes. "Our goal is to provide researchers and reviewers with a tutorial to improve the rigor of the science in future obesity studies,” said Brandon George, Ph.D., statistician in the University of Alabama at Birmingham Office of Energetics. “Investigators who conduct primary research may find the paper useful to read or share with statistical collaborators to obtain a deeper understanding of statistical issues, avoid making the discussed errors, and increase the reproducibility and rigor of the field. Editors, reviewers and consumers will find valuable information allowing them to properly identify these common errors while critically reading the work of others."


The Signal and the Noise: The Problem of Reproducibility
http://cameronneylon.net/blog/the-signal-and-the-noise-the-problem-of-reproducibility/
2016-03-20
popular news
Once again, reproducibility is in the news. Most recently we hear that irreproducibility is irreproducible and thus everything is actually fine. The most recent round was kicked off by a criticism of the Reproducibility Project followed by claim and counter claim on whether one analysis makes more sense than the other. I’m not going to comment on that but I want to tease apart what the disagreement is about, because it shows that the problem with reproducibility goes much deeper than whether or not a particular experiment replicates.


Automatic Benchmark Profiling through Advanced Trace Analysis
https://hal.inria.fr/hal-01292618/document
2016-03-24
VisTrails
Benchmarking has proven to be crucial for the investigation of the behavior and performances of a system. However, the choice of relevant benchmarks still remains a challenge. To help the process of comparing and choosing among benchmarks, we propose a solution for automatic benchmark profiling. It computes unified benchmark profiles reflecting benchmarks’ duration, function repartition, stability, CPU efficiency, parallelization and memory usage. It identifies the needed system information for profile computation, collects it from execution traces and produces profiles through efficient and reproducible trace analysis treatments. The paper presents the design, implementation and the evaluation of the approach. The analysis of the kernel trace follows a workflow implemented using the VisTrails tool.


Failure Is Moving Science Forward
http://fivethirtyeight.com/features/failure-is-moving-science-forward/
2016-03-24
popular news
As science grapples with what some have called a reproducibility crisis, replication studies, which aim to reproduce the results of previous studies, have been held up as a way to make science more reliable. It seems like common sense: Take a study and do it again — if you get the same result, that’s evidence that the findings are true, and if the result doesn’t turn up again, they’re false. Yet in practice, it’s nowhere near this simple.


Reproducibility in density functional theory calculations of solids
http://science.sciencemag.org/content/351/6280/aad3000
2016-03-25
reproducibility study
The scrutiny of the scientific community has also turned to research involving computer programs, finding that reproducibility depends more strongly on implementation than commonly thought. These problems are especially relevant for property predictions of crystals and molecules, which hinge on precise computer implementations of the governing equation of quantum physics. We devised a procedure to assess the precision of DFT methods and used this to demonstrate reproducibility among many of the most widely used DFT codes. 


myExperiment
http://www.myexperiment.org/home
2015-01-01
reproducibility infrastructure
myExperiment is a collaborative environment where scientists can safely publish their workflows and in silico experiments, share them with groups and find those of others. Workflows, other digital objects and bundles (called Packs) can now be swapped, sorted and searched like photos and videos on the Web. 


The Legal Framework for Reproducible Scientific Research: Licensing and Copyright
http://doi.ieeecomputersociety.org/10.1109/MCSE.2009.19
2009-01-01
open access
The code, data structures, experimental design and parameters, documentation, and figures are all important for scholarship communication and result replication. The author proposes the reproducible research standard for scientific researchers to use for all components of their scholarship that should encourage reproducible scientific investigation through attribution, facilitate greater collaboration, and promote engagement of the larger community in scientific learning and discovery.


Sweave
http://www.statistik.lmu.de/~leisch/Sweave/
2002-01-01
reproducibility infrastructure
Sweave is a tool that allows to embed the R code for complete data analyses in latex documents, and is automatically packaged in R installations. The purpose is to create dynamic reports, which can be updated automatically if data or analysis change. Instead of inserting a prefabricated graph or table into the report, the master document contains the R code necessary to obtain it. When run through R, all data analysis output (tables, graphs, etc.) is created on the fly and inserted into a final latex document. The report can be automatically updated if data or analysis change, which allows for truly reproducible research. It does not, however, track provenance. 


Software Tools to Facilitate Research Programming
https://purl.stanford.edu/mb510fs4943
2012-05-28
reproducibility infrastructure
Ph.D. dissertation, Department of Computer Science, Stanford University, 2012: "By understanding the unique challenges faced during research programming, it becomes possible to apply techniques from dynamic program analysis, mixed-initiative recommendation systems, and OS-level tracing to make research programmers more productive. This dissertation characterizes the research programming process, describes typical challenges faced by research programmers, and presents five software tools that I have developed to address some key challenges."


Our approach to replication in computational science
http://ivory.idyll.org/blog/replication-i.html
2012-04-02
reproducible paper
A blog post from C. Titus Brown on how he and his co-authors were able to make a paper they wrote replicable. 


Tools and techniques for computational reproducibility
http://biorxiv.org/content/early/2016/03/17/022707
2016-03-17
reproducibility infrastructure
When reporting research findings, scientists document the steps they followed so that others can verify and build upon the research. When those steps have been described in sufficient detail that others can retrace the steps and obtain similar results, the research is said to be reproducible. Computers play a vital role in many research disciplines and present both opportunities and challenges for reproducibility. With a broad scientific audience in mind, we describe strengths and limitations of each approach, as well as circumstances under which each might be applied. No single strategy is sufficient for every scenario; thus we emphasize that it is often useful to combine approaches.


Reproducibility: Team up with industry
http://www.nature.com/news/reproducibility-team-up-with-industry-1.19551
2016-03-16
news article
The scientific community is bustling with projects to make published results more reliable. Efforts are under way to establish checklists, to revamp training in experimental design, and even to fund disinterested scientists to replicate others' experiments. A more efficient strategy would be to rework current incentives to put less emphasis on high-impact publications, but those systems are entrenched, and public funders and universities are ill-prepared for that scale of change. To catalyse change, industry must step up to the plate. I have learned this first hand, as head of the Structural Genomics Consortium (SGC), a research charity funded by business, government and other charities. If more companies contributed funds and expertise to efforts such as ours, I believe it would create a system that rewards science that is both cutting-edge and reproducible.


Many scientific "truths" are, in fact, false
http://qz.com/638059/many-scientific-truths-are-in-fact-false/
2016-03-13
popular news
In 2005, John Ioannidis, a professor of medicine at Stanford University, published a paper, “Why most published research findings are false,” mathematically showing that a huge number of published papers must be incorrect. He also looked at a number of well-regarded medical research findings, and found that, of 34 that had been retested, 41% had been contradicted or found to be significantly exaggerated. Since then, researchers in several scientific areas have consistently struggled to reproduce major results of prominent studies. By some estimates, at least 51%—and as much as 89%—of published papers are based on studies and experiments showing results that cannot be reproduced.


Replication Mirror
http://www.psi-chology.com/replication-mirror/
2016-03-14
replication study
A satirical piece detailing the replication and reproducibility crisis in Psychology. 


The Quest for Reproducible Science: Issues in Research Transparency and Integrity
http://www.ala.org/alcts/events/ac/2016/reproduciblescience
2016-03-09
reproducibility conference
A pre-conference event of the American Library Association's annual conference: "The credibility of scientific findings is under attack. While this crisis has several causes, none is more common or correctable than the inability to replicate experimental and computational research. This preconference will feature scholars, librarians, and technologists who are attacking this problem through tools and techniques to manage data, enable research transparency, and promote reproducible science. Attendees will learn strategies for fostering and supporting transparent research practices at their institutions."


Evaluating replicability of laboratory experiments in economics
http://science.sciencemag.org/content/early/2016/03/02/science.aaf0918
2016-03-03
popular news, replication study
The reproducibility of scientific findings has been called into question. To contribute data about reproducibility in economics, we replicate 18 studies published in the American Economic Review and the Quarterly Journal of Economics in 2011-2014. All replications follow predefined analysis plans publicly posted prior to the replications, and have a statistical power of at least 90% to detect the original effect size at the 5% significance level. We find a significant effect in the same direction as the original study for 11 replications (61%); on average the replicated effect size is 66% of the original. The reproducibility rate varies between 67% and 78% for four additional reproducibility indicators, including a prediction market measure of peer beliefs.


Research Software Sustainability: Report on Knowledge Exchange workshop
http://repository.jisc.ac.uk/6332/1/Research_Software_Sustainability_Report_on_KE_Workshop_Feb_2016_FINAL.pdf
2016-03-03
reproducibility report, reproducibility conference
The report introduces software sustainability, provides definitions, clearly demonstrates that software is not the same as data and illustrates aspects of sustainability in the software lifecycle. The recommendations state that improving software sustainability requires a number of changes: some technical and others societal, some small and others significant. We must start by raising awareness of researchers' reliance on software. This goal will become easier if we recognise the valuable contribution that software makes to research and reward those people who invest their time into developing reliable and reproducible software. 


Psychology’s reproducibility problem is exaggerated – say psychologists
http://www.nature.com/news/psychology-s-reproducibility-problem-is-exaggerated-say-psychologists-1.19498
2016-03-03
popular news, replication study
In August 2015, a team of 270 researchers reported the largest ever single-study audit of the scientific literature. Led by Brian Nosek, executive director of the Center for Open Science in Charlottesville, Virginia, the Reproducibility Project attempted to replicate studies in 100 psychology papers. According to one of several measures of reproducibility, just 36% could be confirmed; by another statistical measure, 47% could. Not so fast, says Gilbert. Because of the way the Reproducibility Project was conducted, its results say little about the overall reliability of the psychology papers it tried to validate, he argues. "The number of studies that actually did fail to replicate is about the number you would expect to fail to replicate by chance alone — even if all the original studies had shown true effects."


ReproZip Featured on Library of Congress Blog Post
https://blogs.loc.gov/digitalpreservation/2016/02/blurred-lines-shapes-and-polygons-part-1-an-ndsr-ny-project-update/
2016-02-12
ReproZip, popular news
ReproZip was featured in a post on the Library of Congress's digital preservation blog, the Signal. The author, Genevieve Havemeyer-King, writes "ReproZip is a tool being developed at NYU "aimed at simplifying the process of creating reproducible experiments from command-line executions", and could be something to consider as an alternative to many costly web-archiving services for preservation of internet-based projects and applications."


University of Washington's eScience Institute Guidelines for Reproducible & Open Science
http://uwescience.github.io/reproducible/guidelines.html
2016-03-01
reproducibility guidelines
Our working definition for reproducible research is that a research result can be replicated by another investigator. Our focus is data science and the reproducibility of computational studies and/or analysis of digital data. This note summarizes best practices to facilitate reproducible research in data science (and computational science more generally). It is expected that all research conducted with funding from the DSE will be performed in accordance with these guidelines to the extent possible.


ACM SIGMOD 2016 Reproducibility Guidelines
http://db-reproducibility.seas.harvard.edu/
2016-03-01
reproducibility guidelines
SIGMOD Reproducibility has three goals: Highlight the impact of database research papers; Enable easy dissemination of research results; Enable easy sharing of code and experimentation set-ups. In short, the goal is to assist in building culture where sharing results, code, and scripts of database research is the norm rather than the exception. The challenge is to do this efficiently, which means building technical expertise on how to do better research via creating repeatable and shareable research. The SIGMOD Reproducibility Committee is here to help you with this.


Transparency and Openness Promotion (TOP) Guidelines
https://cos.io/top/
2016-03-01
reproducibility guidelines
Transparency, open sharing, and reproducibility are core features of science, but not always part of daily practice. Journals can increase transparency and reproducibility of research by adopting the TOP Guidelines. TOP includes eight modular standards, each with three levels of increasing stringency. Journals select which of the eight transparency standards they wish to adopt for their journal, and select a level of implementation for the selected standards. These features provide flexibility for adoption depending on disciplinary variation, but simultaneously establish community standards. 


Reproducibility of Research Guide by the Eccles Health Sciences Library (EHSL) at University of Utah
http://campusguides.lib.utah.edu/reproducibility
2016-03-01
research guide
This LibGuide from the University of Utah outlines some first steps, tutorials, and toolkits related to making research reproducible, with a strong focus on quantitative and computational research.


Statistical Challenges in Assessing and Fostering the Reproducibility of Scientific Results: Summary of a Workshop
http://www.nap.edu/read/21915/chapter/1
2016-03-01
reproducibility conference, reproducibility report
The workshop summarized in this report was designed not to address the social and experimental challenges but instead to focus on the latter issues of improper data management and analysis, inadequate statistical expertise, incomplete data, and difficulties applying sound statistical inference to the available data.


ReproZip Demo Accepted at SIGMOD 2016
http://vida-nyu.github.io/reprozip/news.html#sigmod-demo-2016
2016-02-27
ReproZip, reproducibility talk, reproducibility infrastructure
A ReproZip demo has been accepted at SIGMOD 2016: "ReproZip: Computational Reproducibility With Ease." F. Chirigati, R. Rampin, D. Shasha, and J. Freire.


Shape Modeling International (SMI 2016) Introduces Reproducibility Award
http://www.geometrysummit.org/smi2016/index.html
2016-02-29
reproducibility conference
This year, also SMI will introduce an Award for Reproducibility to be granted to authors of accepted papers who are willing to provide a complete open-source implementation of their algorithm. The reproducibility stamp does not affect the reviewing process or the requirements for your submission to be accepted. The awarded papers will receive an additional 5 to 10 minutes in their presentation to give a live demo and will be recognized during the SMI closing ceremony. More information on the web site soon.


Janiform Papers Demo (pdbf: portable database files)
https://www.youtube.com/watch?v=f4iKwdERXhI&feature=youtu.be
2016-02-29
reproducibility infrastructure, reproducible paper
PDBF documents are a hybrid format. They are a valid PDF and a valid HTML page at the same time. You can now optionally add an VirtualBox OVA file with a complete operating system to the PDBF document. Yes, this means that the resulting file is a valid PDF, HTML, and OVA file at the same time. If you change the file extension to PDF and open it with an PDF viewer, you can see the static part of the document.


How Many Replication Studies are Enough?
http://www.nature.com/news/how-many-replication-studies-are-enough-1.19461
2016-02-26
replication study, news article
Researchers on social media ask at what point replication efforts go from useful to wasteful. The problem of irreproducibility in science has gained widespread attention, but one aspect that is discussed less often is how to find the right balance between replicating findings and moving a field forward from well-established ones.


A Bayesian Perspective on the Reproducibility Project: Psychology
http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0149794
2016-02-26
replication study, reproducible paper
We revisit the results of the recent Reproducibility Project: Psychology by the Open Science Collaboration. We compute Bayes factors—a quantity that can be used to express comparative evidence for an hypothesis but also for the null hypothesis—for a large subset (N = 72) of the original papers and their corresponding replication attempts.


A Practical Guide for Improving transparency and Reproducibility in Neuroimaging Research
http://dx.doi.org/10.1101/039354
2016-02-14
reproducibility guidelines 
Recent years have seen an increase in alarming signals about the lack of replicability in neuroscience, psychology, and other related fields. To avoid a widespread crisis in our field and consequent loss of credibility in the public eye, we need to improve how we do science. This article aims to be a practical guide for researchers at any stage of their careers that will help them make their research more reproducible and transparent while minimizing the additional effort that this might require. The guide covers three major topics in open science (data, code, and publications) and offers practical advice as well as highlighting advantages of adopting more open research practices that go beyond improved transparency and reproducibility.


A New Group Is Dedicated To Double-Checking Scientists' Work
http://www.newsy.com/videos/a-new-group-is-dedicated-to-double-checking-scientists-work/
2016-02-12
reproducibility infrastructure, popular news
For the past decade, scientists have been worried about the so-called replication crisis. Enter the Preclinical Reproducibility and Robustness channel. The website launched the first week in February with the goal of publishing the results of replication studies. The journal wants to keep scientists accountable for their work.


BioPolicy Summit tackles reproducibility of science issues
https://biodesign.asu.edu/news/biopolicy-summit-tackles-reproducibility-science-issues
2016-02-12
reproducibility conference, news article, reproducibility talk
The 2016 GBSI Summit—"Research Reproducibility: Innovative Solutions to Drive Quality" welcomed premiere life science thought leaders, including Arizona State University biomarker researcher Joshua LaBaer, MD, PhD, and science correspondent and moderator Richard Harris (currently on leave from National Public Radio as a visiting scholar this spring at Arizona State University), to explore the driving forces and profound impacts behind the issues.


It’s a kind of magic: how to improve adherence to reporting guidelines
http://blogs.biomedcentral.com/bmcseriesblog/2016/02/12/kind-magic-improve-adherence-reporting-guidelines/
2016-02-12
popular news, news article, reproducible journal
Finding a relevant reporting guideline for a study can be very difficult. Here we introduce a pilot experiment starting for some of the BMC-series journals which aims to overcome this issue.


Friday: Reusing Data and Making Your Data Reusable
http://data-services.hosting.nyu.edu/updates/lyd16-friday/
2016-02-12
research guide
This blog post is apart of the Love Your Data campaign #LYD16, a global and cross-institution awareness campaign for open data, research reproducibility, and research data management. This post features ReproMatch and ReproZip as important tools for achieving reproducibility. 


A Noob's Guide to Reproducibility
http://www.stat.berkeley.edu/~stark/Seminars/reproNE16.htm#1
2016-02-10
reproducibility guidelines, reproducibility talk
A presentation by Philip B. Stark of University of California at Berkeley that gives a great 101-style look into what the everyday researcher can do to make their science more reproducible. 


Science is "show me," not "trust me"
http://www.bitss.org/2015/12/31/science-is-show-me-not-trust-me/
2016-02-10
reproducibility guidelines, popular news
A blog post from Philip B. Stark, Associate Dean of the Division of Mathematical and Physical Sciences, UC Berkeley Professor of Statistics, and winner of one of BITSS’ Leamer-Rosenthal Prizes for Open Social Science. This post discuss the core elements of reproducibility; its principles and practices.


Startup unveils tools to improve trial reproducibility
http://www.mmm-online.com/dataanalytics/startup-unveils-tools-to-improve-trial-reproducibility/article/471977/
2016-02-09
reproducibility infrastructure, popular news, news article
Elemental Machines, which develops smart laboratory technology, launched a new suite of tools that that measure environmental variables such as temperature and humidity—both of which are not traditionally accounted for in scientific experiments. By “debugging” the lab environment, the company believes it can improve experimental reproducibility, therefore reducing the time and cost of marketing new drugs and therapies. Elemental Machines recently raised $2.5 million in seed capital to support the development of the new suite of tools, which is called the EM Suite.


Co-founder of Center for Scientific Integrity speaks Feb. 19 about issues in scholarly publishing
http://www.montana.edu/news/15963/co-founder-of-center-for-scientific-integrity-speaks-feb-19-about-issues-in-scholarly-publishing
2016-02-05
open access, reproducibility talk
Adam Marcus, cofounder of Retraction Watch and the Center for Scientific Integrity, will give a free lecture about issues in scholarly science publishing at 4 p.m. Friday, Feb. 19, in 103 Reid Hall at Montana State University.


Misfit Founders Raise $2.5M to 'Debug the Physical World' With New Startup
http://bostinno.streetwise.co/2016/02/03/misfit-wearables-founders-raise-2-5m-for-elemental-machines/
2016-02-03
reproducibility infrastructure, popular news
Elemental Machines, a venture based in Boston and San Francisco, has come out of stealth mode. The startup says it's raised $2.5 million in seed from investors including Founders Fund’s FF Angel, PayPal co-founder Max Levchin and Project 11 Ventures. And now it’s ready to change the way our world does science, providing the infrastructure that will ensure experiment reproducibility for researchers.


Reproducibility: A tragedy of errors
http://www.nature.com/news/reproducibility-a-tragedy-of-errors-1.19264
2016-02-03
news article, reproducibility report
Mistakes in peer-reviewed papers are easy to find but hard to fix, report David B. Allison and colleagues: "In the course of assembling weekly lists of articles in our field, we began noticing more peer-reviewed articles containing what we call substantial or invalidating errors. These involve factual mistakes or veer substantially from clearly accepted procedures in ways that, if corrected, might alter a paper's conclusions."


ReproZip Poster Accepted at FORCE2016
https://www.force11.org/meetings/force2016/program/agenda
2016-01-27
ReproZip, reproducibility talk, reproducibility infrastructure
Fernando Chirigati and Remi Rampin's poster "Enhancing Scholarly Communication with ReproZip" was recently accepted at FORCE2016, a conference from FORCE11 a community of scholars, librarians, archivists, publishers and research funders that has arisen organically to help facilitate the change toward improved knowledge creation and sharing. 


GBSI Doubles Down on Research Reproducibility at Annual BioPolicy Summit and Webcast in Washington, DC, February 9th
http://www.newswise.com/articles/gbsi-doubles-down-on-research-reproducibility-at-annual-biopolicy-summit-and-webcast-in-washington-dc-february-9th
2016-01-27
reproducibility conference, news article, reproducibility talk
The Summit will also introduce GBSI’s Reproducibility2020, an action plan for the biomedical research community to significantly improve the quality of research by 2020 targeting: 1) improved validation and standardization of biological reagents; 2) better tools and technologies to expand open access for reporting and sharing protocols and data; and 3) increased training that emphasizes rigorous study design and practice.


Reproducibility from a Mostly Selfish Point of View
https://discuss.ropensci.org/t/slides-and-some-thoughts-on-a-talk-about-reproducibility/294
2016-01-27
popular news, news article, reproducibility talk
A talk given by Noam Ross: "Why was, as the title suggests, primarily focused on the benefits of reproducibility to us, and I proceeded from avoiding negatives (risk avoidance) to creating positives (more impact). In How I tried to be very high-level, talking about major concepts in reproducibility, and then talking generally about the tools that I have used for each, emphasizing that they may not be the right tools for everyone. Then we had a discussion about the most promising areas and tools to start with."


New Shotgun Mass Spec Workflow Could Improve Reproducibility of Protein Quantitation in DDA
https://www.genomeweb.com/proteomics-protein-research/new-shotgun-mass-spec-workflow-could-improve-reproducibility-protein
2016-01-21
reproducibility report, news article, reproducibility infrastructure
Researchers at Sweden's Karolinska Institute and Royal Institute of Technology have developed a new data analysis workflow for shotgun mass spec that could help improve the technique's quantitative reproducibility. Detailed in a paper published this month in Molecular & Cellular Proteomics, the approach uses a new quality scoring system that allows for more reliable recovery of missing data points across multiple mass spec runs.


noWorkflow Demo Video Released
https://www.youtube.com/watch?v=lyJnbwdArJM
2016-01-20
noWorkflow, reproducibility infrastructure
A video demonstrating noWorkflow, a non-intrusive tool that allows researchers to capture a variety of provenance information and utilize the analyses it supports, including graph-based visualization, differencing over provenance trails, and inference queries.


FASEB Issues Recommendations on Reproducibility
http://www.faseb.org/Resources-for-the-Public/News-Room/Article-Detail-View/tabid/1014/ArticleId/1251/FASEB-Issues-Recommendations-on-Reproducibility.aspx
2016-01-14
reproducibility report, news article, reproducibility guidelines
Today the Federation of American Societies for Experimental Biology (FASEB) issued Enhancing Research Reproducibility, a set of recommendations aimed to promote the reproducibility and transparency of biomedical and biological research. 


Lecture: A Noob's Guide to Reproducibility
http://bids.berkeley.edu/events/noobs-guide-reproducibility
2016-01-11
reproducibility talk
Lecture on January 25, 2016; 4:00pm to 5:00pm; 3110 Etcheverry Hall at Berkely Institute of Data Science. What does it mean to work reproducibly and transparently? Why bother? Whom does it benefit, and how? What will it cost me? What work habits will I need to change? Will I need to learn new tools? What resources help? What's the simplest thing I can do to make my work more reproducible? How can I move my discipline, my institution, and science as a whole towards reproducibility?


Upcoming Webinar: Scientific Rigor and Data Reproducibility
https://www.sfn.org/news-and-calendar/news-and-calendar/news/professional-development/upcoming-webinar-scientific-rigor-and-data-reproducibility
2016-01-11
reproducibility talk, reproducibility guidelines, news article, popular article
The topics of scientific rigor and data reproducibility have been increasingly covered in the scientific and mainstream media, and are being addressed by publishers, professional organizations, and funding agencies, including NIH. This webinar – the first in a series titled Training Modules to Enhance Data Reproducibility (TMEDR) – will address topics of scientific rigor as they pertain to pre-clinical neuroscience research. 


R's role in science breakthrough: reproducibility of psychology studies
http://blog.revolutionanalytics.com/2016/01/rs-role-in-science-breakthrough-reproducibility-of-psychology-studies.html
2016-01-08
popular article, news article, reproducibility infrastructure
R is a natural fit for a reproducibility project like this: as a scripting language, the R script itself provides a reproducible documentation of every step of the process. (Revolution R Open, Microsoft's enhanced R distribution, additionally includes features to facilitate reproducibility when using R packages.) The R script used for the psychology replication project describes and executes the process for checking the results of the papers.


"PEOPLE LIKE STORIES" A SHORT FILM ABOUT REPRODUCIBILITY
https://politicalsciencereplication.wordpress.com/2016/01/07/people-like-stories-a-short-film-about-reproducibility/
2016-01-07
reproducibility talk, popular news
We need mathematical help to tell the difference between a real discovery and the illusion of one. Fellow of the Royal Society and future President of the Royal Statistical Society, Sir David Spiegelhalter visits Dr Nicole Janz  to discuss reproducibility in scientific publications.


A Proactive Approach to Reproducibility with Evidence-Based Research on Research
https://www.plos.org/a-proactive-approach-to-reproducibility-with-evidence-based-research-on-research/
2016-01-06
reproducible journal, news article
The new Meta-Research Section in PLOS Biology is not the only example of how PLOS strives to improve the scientific endeavor through innovative communication efforts. PLOS has always recognized that publication of studies that reproduce published work or null results, either confirming or refuting the original result, is essential for progress in research. In fact, the largest journal at PLOS, PLOS ONE, is one of only a handful of publications that actively encourage these types of submissions with The Missing Pieces Collection.


Reproducibility Project Named Among Top Scientific Achievements of 2015
https://www.psychologicalscience.org/index.php/publications/observer/obsonline/reproducibility-project-named-among-top-scientific-achievements-of-2015.html
2016-01-05
reproducible journal, replication study
The journal Science has named a major attempt to replicate 100 papers published in top-tier psychology journals as one of the "breakthroughs of the year" for 2015.


Why Scientists Need to Fail
http://www.psmag.com/nature-and-technology/science-needs-to-fail
2015-12-22
popular news
As researchers think about how to improve reproducibility, it's important to remember that failure is a crucial part of the scientific process.


Winning Video from GBSI #authenticate Campaign Will Promote Reproducibility Among Younger Generation of Biomedical Researchers
http://www.newswise.com/articles/winning-video-from-gbsi-authenticate-campaign-will-promote-reproducibility-among-younger-generation-of-biomedical-researchers
2015-12-17
reproducibility conference
The Global Biological Standards Institute (GBSI) today announced the winner of its #authenticate video competition to promote cell authentication in biomedical research is Michael Ge, from West Covina, California.


Reproducibility at SC16 with the Student Cluster Competition
http://www.nist.gov/itl/ssd/is/upload/NRE-2015-00-SC16SCC_CfP_slide.pdf
2015-12-17
reproducibility conference
Replication and reproducibility of experimental computer science results in peer-reviewed paper is gaining relevance in the HPC community. SC, the leading conference in the field, wants to promote and support replication and reproducibility through a new initiative that aims to integrate aspects of past technical papers into the Student Cluster Competition (SCC). SC16 invites authors of technical papers accepted at past SC conferences, including SC15, to submit proposals for case studies based on applications and tests in their SC paper that can be transformed into benchmarks for the SCC. This initiative provides SC authors with the unique opportunity to further promote their published research as an example of replicable and reproducible experimental computer science.


Clinical Genetics Has a Big Problem That's Affecting People's Lives
http://www.theatlantic.com/science/archive/2015/12/why-human-genetics-research-is-full-of-costly-mistakes/420693/
2015-12-16
popular news
Over the last decade, there’s been a lot of talk about reproducibility problems in science — about published results that turn out to be false alarms. In fields like psychology, neuroscience, and cell biology, these errors can send scientists down unproductive paths, waste time and money, and pollute headlines with misleading claims. "But I get much more exercised about reproducibility problems in clinical genetics, because those have massive and real-time consequences for thousands of families," says MacArthur.


Emphasize Sex in Research, orders National Institutes of Health
http://synapse.ucsf.edu/articles/2015/12/16/emphasize-sex-research-orders-national-institutes-health
2015-12-16
popular news, news article, reproducibility guidelines
While experiments may be published even in a top scientific journal, other researchers who attempt to repeat the same experiments under the same conditions often find contradicting results. As a measure of this, a recent study attempted to reproduce psychology publications and successfully replicated only 39 out of 100 studies. It turns out that excluding sex in experimental design may have contributed to reproducibility issues. Furthermore, sex can also have a biological impact on our scientific understanding and influence how well early biological studies translate into advances in human medicine.


Year in review: Scientists tackle the irreproducibility problem
https://www.sciencenews.org/article/year-review-scientists-tackle-irreproducibility-problem
2015-12-15
popular news, news article, reproducibility report
Experimental results that don’t hold up to replication have caused consternation among scientists for years, especially in the life and social sciences (SN: 1/24/15, p. 20). In 2015 several research groups examining the issue reported on the magnitude of the irreproducibility problem. The news was not good.


Reproducibility in Medical IVA
https://osf.io/5afwm/
2015-12-09
reproducibility study
Project on Reproducibility and Robustness of the Empirical Instrumental Variables Literature in Medicine. 


Reproducibility: Experimental mismatch in neural circuits
http://www.nature.com/nature/journal/vaop/ncurrent/full/nature16323.html
2015-12-09
news article, replication study
The finding that acute and chronic manipulations of the same neural circuit can produce different behavioural outcomes poses new questions about how best to analyse these circuits.


Translation, cultural adaptation and reproducibility of the Oxford Shoulder Score questionnaire for Brazil, among patients with rheumatoid arthritis.
http://www.ncbi.nlm.nih.gov/pubmed/26648280
2015-12-08
replication , news article
Although shoulder questionnaires validated for Brazil do exist, none of them are aimed at populations with rheumatic disease. We believe that the Oxford Shoulder Score (OSS) may be useful in this population. The objective of this study was to translate the OSS, adapt it to Brazilian culture and test its reproducibility.


Letting Out Steam: Reproducibility Problems
https://www.digital-science.com/blog/perspectives/letting-out-steam-reproducibility-problems/
2015-12-08
news article, popular news
The first part of the STM innovations seminar focused on the problems of reproducibility in science. For some years now, there have been voices of concern noting that when previously reported results are tested, the data very often doesn’t come out the same way. During the seminar, Andrew Hufton of Scientific Data went so far as to state that progress in the pharmaceutical sciences is being held back by lack of reliability in the basic literature. 


Big problems for common fMRI thresholding methods
http://reproducibility.stanford.edu/big-problems-for-common-fmri-thresholding-methods/
2015-12-08
news article, reproducibility guidelines
Stanford Center for Reproducible Neuroscience: A new preprint has been posted to the ArXiv that has very important implications and should be required reading for all fMRI researchers.  Anders Eklund, Tom Nichols, and Hans Knutson applied task fMRI analyses to a large number of resting fMRI datasets, in order to identify the empirical corrected “familywise” Type I error rates observed under the null hypothesis for both voxel-wise and cluster-wise inference.  What they found is shocking: While voxel-wise error rates were valid, nearly all cluster-based parametric methods (except for FSL’s FLAME 1) have greatly inflated familywise Type I error rates.  This inflation was worst for analyses using lower cluster-forming thresholds (e.g. p=0.01) compared to higher thresholds, but even with higher thresholds there was serious inflation.  This should be a sobering wake-up call for fMRI researchers, as it suggests that the methods used in a large number of previous publications suffer from exceedingly high false positive rates (sometimes greater than 50%).  


How do we fix bad science?
https://cosmosmagazine.com/society/how-do-we-fix-bad-science
2015-12-07
news article, reproducibility report
Independently verifying research can help science regain its credibility, argues Laurie Zoloth. His paper: "Why Most Published Research Findings Are False", was published in August 2005, in PLOS Medicine. It became one of the journal’s most-cited articles. While climate sceptics, anti-vaccination campaigners and the rest of the pseudo-science community have dined out on this paper, arguably it has been a shot in the arm for science. 


ReproZip 1.0.3 released
https://github.com/ViDA-NYU/reprozip/releases/tag/1.0.3
2015-12-02
ReproZip
A new version of ReproZip has been released, adding some bugfixes and options to pass environment variables to the experiment.


Cancer reproducibility project scales back ambitions
http://www.nature.com/news/cancer-reproducibility-project-scales-back-ambitions-1.18938
2015-12-02
popular news, news article
The Reproducibility Project: Cancer Biology aims to get a better, quantitative estimate of the reproducibility of important work and to understand the challenges such efforts present. Begun in 2013, the project is run jointly by the Center for Open Science (COS) in Charlottesville, Virginia, and Science Exchange in Palo Alto, California. 


Reproducibility of Research: Get Started
http://campusguides.lib.utah.edu/reproducibility
2015-12-01
research guide
A research guide from the University of Utah on making research reproducible.


Brian Nosek on the Reproducibility Project
http://www.econtalk.org/archives/2015/11/brian_nosek_on.html
2015-11-16
popular news, reproducibility report
Brian Nosek of the University of Virginia and the Center for Open Science talks with EconTalk host Russ Roberts about the Reproducibility Project.


Promises, Promises, and Cell Lines: Life Sciences Researchers Talk About the Obvious Solution—Cell-Line Authentication—but They Fail To Implement It
http://www.genengnews.com/gen-articles/promises-promises-and-cell-lines/5631/
2015-11-15
reproducibility report, news article
According to a 2013 report from the American Association for the Advancement of Science, $115 billion is spent annually in the United States on life science research. Fifty percent of this total is spent on preclinical research, half of which—$28 billion—is not reproducible.


Speaking of Research Integrity 
http://www.the-scientist.com/?articles.view/articleNo/44424/title/Speaking-of-Research-Integrity/
2015-11-06
news article, reproducibility report
Panelists discuss reproducibility, data-sharing, and encouraging early-career researchers at this year’s World Science Forum.


ReproZip Demo Tutorial Video
https://youtu.be/-zLPuwCHXo0
2015-11-05
ReproZip
This is a demo video showing how to pack and unpack experiments with ReproZip.


Bioethics and the reproducibility crisis
http://www.bioedge.org/bioethics/bioethics-and-the-reproducibility-crisis/11632
2015-10-31
news article, reproducibility report, replication study
According to the mayor of Chicago, Rahm Emanuel, who is linked to bioethics through his bioethicist brother Ezekiel Emanuel, "You never let a serious crisis go to waste." In this case the crisis is the reproducibility of published results in the biological and medical sciences. According to a recent comment in Nature, "An unpublished 2015 survey by the American Society for Cell Biology found that more than two-thirds of respondents had on at least one occasion been unable to reproduce published results. Biomedical researchers from drug companies have reported that one-quarter or fewer of high-profile papers are reproducible."


Improving the reproducibility of biomedical research
http://www.bbsrc.ac.uk/news/policy/2015/151029-pr-improving-reproducibility-of-biomedical-research/
2015-10-29
news article, reproducibility report
The Academy of Medical Sciences has published a new joint report on how the reproducibility and reliability of research can be improved. Recent reports in the general and scientific media show there is increasing concern within the biomedical research community about the lack of reproducibility of key research findings.


Reproducibility in science — where the MRC comes in
http://www.insight.mrc.ac.uk/2015/10/29/reproducibility-in-science-where-the-mrc-comes-in/
2015-10-29
news article, reproducibility report
The MRC and a group of partner organisations have today published a report and joint statement  about the reproducibility and reliability of research, and what can be done to improve them. Here, Jim Smith, MRC Deputy Chief Executive and Director of Strategy, thinks about how discussions of reproducibility offer us the opportunity to improve the way science is done.


MSDSE Reproducibility Zotero Bibliography
https://www.zotero.org/groups/msdse-reproducibility
2015-10-05
reproducibility bibliography
This group is for sharing reproducibility related citeable resources within the Moore and Sloan Data Science Environments reproducibility working group effort.


New Study: Scientific Researchers Are Not Always Reliable
http://www.utahpeoplespost.com/2015/08/new-study-scientific-researches-are-not-always-reliable/
2015-08-29
replication study, news article
Researchers tested the credibility of past investigations reaching the conclusion of a new study: scientific researches are not always reliable. Few of the past studies could be replicated showing that some researches are either too biased or too distinctive to make a statement in history.


The Results of the Reproducibility Project Are In. They’re Not Good.
http://chronicle.com/article/The-Results-of-the/232695
2015-08-28
news article, popular news, replication study
A decade ago, John P.A. Ioannidis published a provocative and much-discussed paper arguing that most published research findings are false. It’s starting to look like he was right.


Massive Study Reports Challenges in Reproducing Published Psychology Findings
https://news.virginia.edu/content/massive-study-reports-challenges-reproducing-published-psychology-findings
2015-08-27
replication study, news article
A study that sought to replicate 100 findings published in three prominent psychology journals has found that, across multiple criteria, independent researchers could replicate less than half of the original findings. In some cases this may call into question the validity of some scientific findings, but it may also point to the difficulty of conducting effective replications and achieving reproducible results.


Reproducibility blues
https://dx.doi.org/10.15252/embj.201570090
2015-04-11
popular news
Research findings advance science only if they are significant, reliable and reproducible. Scientists and journals must publish robust data in a way that renders it optimally reproducible. Reproducibility has to be incentivized and supported by the research infrastructure but without dampening innovation.


Program Seeks to Nurture 'Data Science Culture' at Universities
http://bits.blogs.nytimes.com/2013/11/12/program-seeks-to-nurture-data-science-culture-at-universities/?_php=true&_type=blogs&smid=fb-share&_r=1
2013-11-12
popular news, news article, data science
In collaboration with the University of Washington (UW) and Berkeley, and under the sponsorship of the Moore and Sloan foundations, NYU is working on a new initiative to 'harness the potential of data scientists and big data'. As part of this initiative, we aim to increase awareness of sharing, preservation, provenance, and reproducibility best practices across UW, NYU, Berkeley campuses and encourage their adoption. 


Open-Access Deal for Particle Physics
http://www.nature.com/news/open-access-deal-for-particle-physics-1.11468
2012-09-24
open access
The entire field of particle physics is set to switch to open-access publishing, a milestone in the push to make research results freely available to readers.


Junk science? Most preclinical cancer studies don't replicate
http://www.readthehook.com/103149/junk-science-most-preclinical-cancer-studies-dont-replicate
2012-04-06
replication study, news article
When a cancer study is published in a prestigious peer-reviewed journal, the implication is the findings are robust, replicable, and point the way toward eventual treatments. Consequently, researchers scour their colleagues' work for clues about promising avenues to explore. Doctors pore over the pages, dreaming of new therapies coming down the pike. Which makes a new finding that nine out of 10 preclinical peer-reviewed cancer research studies cannot be replicated all the more shocking and discouraging.


The Database Experiments Repository (DBXR) is Online
http://www.dbxr.org/
2012-01-01
reproducible journal, reproducibility infrastructure
This site serves as a repository for experiments related to database research. Currently, it supports the submission and review of results published at PVLDB and ACM Sigmod.


SIGMOD Repeatability Effort
http://effdas.itu.dk/repeatability/tuning.html
2012-01-01
reproducibility infrastructure, reproducible papers, case studies, VisTrails
As part of this project, in collaboration with Philippe Bonnet, we are using (and extending) our infrastructure to support the SIGMOD Repeatability effort. Below are some case studies that illustrate how authors can create provenance-rich and reproducible papers, and how reviewers can both reproduce the experiments and perform workability tests: packaging an experiment on a distributed database system (link in title).


How Bright Promise in Cancer Testing Fell Apart
http://www.nytimes.com/2011/07/08/health/research/08genes.html
2011-07-11
popular news, news article
Research at Duke University in genomics that involved fighting cancer by looking for gene patterns that would determine which drugs would best attack a particular cancer (no more trial-and-error treatment, considered a breakthrough). This research turned out to be wrong, due to flaws in the research (found by statisticians); if the research was reproducible, errors could have been found earlier and the patients could have continued their treatment. 


It’s Science, but Not Necessarily Right
http://www.nytimes.com/2011/06/26/opinion/sunday/26ideas.html?_r=2
2011-06-25
popular news, news article
NY article discussing the issues with scientific reproducibility: "Why? One simple answer is that it takes a lot of time to look back over other scientists’ work and replicate their experiments. Scientists are busy people, scrambling to get grants and tenure. As a result, papers that attract harsh criticism may nonetheless escape the careful scrutiny required if they are to be refuted."


Galois Conjugates of Topological Phases
http://arxiv.org/abs/1106.3267
2011-06-16
reproducible paper, VisTrails
Professor Matthias Troyer (ETH Zurich) and his collaborators have published a number of papers whose results are fully reproducible. He is using VisTrails to both carry out the experiments and to package them for publication. 


The ALPS Project Release 2.0: Open Source Software for Strongly Correlated Systems
http://arxiv.org/pdf/1101.2646.pdf
2011-05-23
reproducible paper, VisTrails
Professor Matthias Troyer (ETH Zurich) and his collaborators have published a number of papers whose results are fully reproducible. He is using VisTrails to both carry out the experiments and to package them for publication. 


Reproducible Research in the Journal of Biostatistics
http://magazine.amstat.org/blog/2011/01/01/scipolicyjan11
2011-01-01
reproducible journal
The journal Biostatistics has an associate editor for reproducibility who can assign grades of merit to conditionally accepted papers: D: data are available, C: code is available, and R: the AE could run the code and reproduce the results without much effort.


Nobel Laureate Retracts Two Papers Unrelated to Her Prize
http://www.nytimes.com/2010/09/24/science/24retraction.html?_r=1&emc=eta1
2010-09-23
retraction
Linda B. Buck, who shared the 2004 Nobel Prize in Physiology or Medicine for deciphering the workings of the sense of smell, has retracted two scientific papers after she and her colleagues were unable to repeat the findings.
